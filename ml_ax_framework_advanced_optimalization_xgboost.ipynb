{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost hyperparameter optimalization using Ax framework\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) belongs to a family of boosting algorithms and uses the gradient boosting (GBM) framework. Boosting is a sequential technique which works on the principle of an ensemble. It combines a set of weak learners and delivers improved prediction accuracy.\n",
    "\n",
    "Ax is an open-source package from PyTorch that helps you find a minima for any function over the range of parameters. One of the useful ML applications is to find the best hyperparameters for training a model to achieve minimal loss.\n",
    "\n",
    "Sources:\n",
    "- https://xgboost.readthedocs.io/en/latest/tutorials/model.html\n",
    "- https://medium.com/@juniormiranda_23768/ensemble-methods-tuning-a-xgboost-model-with-scikit-learn-54ff669f988a\n",
    "- https://hackernoon.com/want-a-complete-guide-for-xgboost-model-in-python-using-scikit-learn-sc11f31bq\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "- https://www.kaggle.com/stuarthallows/using-xgboost-with-scikit-learn\n",
    "- https://github.com/dmlc/xgboost/blob/master/demo/guide-python/sklearn_examples.py\n",
    "- https://ax.dev/docs/core.html\n",
    "- https://github.com/facebook/Ax/blob/master/tutorials/gpei_svm.ipynb\n",
    "- https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "- https://towardsdatascience.com/rocking-hyperparameter-tuning-with-pytorchs-ax-package-1c2dd79f2948\n",
    "- https://www.kaggle.com/nanomathias/bayesian-optimization-of-xgboost-lb-0-9769\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting without default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "# Generate data for binary classification\n",
    "X, y = datasets.make_hastie_10_2(n_samples=15000, random_state=1)\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "# map labels from {-1, 1} to {0, 1}\n",
    "labels, y = np.unique(y, return_inverse=True)\n",
    "\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.8806923076923077\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# parameters: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst\n",
    "# auc: Area under the curve\n",
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "# learn model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "# predict the data\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "# get accuracy of prediction\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print ('accuracy_score:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimalization of XGBoost using Ax optimalization framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see: https://github.com/facebook/Ax/blob/master/tutorials/gpei_svm.ipynb\n",
    "\n",
    "from ax import (\n",
    "    ParameterType,\n",
    "    RangeParameter,\n",
    "    SearchSpace,\n",
    "    SimpleExperiment,\n",
    "    modelbridge,\n",
    "    models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evaluation function takes in a parameterization (set of parameter values) \n",
    "# and computes all the metrics needed in optimization.\n",
    "# It should output a dictionary of metric names to tuple of mean and standard error.\n",
    "\n",
    "def xgb_evaluation_function(\n",
    "    parameterization, # dict of parameter names to values of those parameters\n",
    "    weight=None, # required by the evaluation function signature\n",
    "):\n",
    "    xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42,\n",
    "                                  colsample_bylevel=parameterization[\"colsample_bylevel\"],\n",
    "                                  colsample_bytree=parameterization[\"colsample_bytree\"],\n",
    "                                  gamma=parameterization[\"gamma\"],\n",
    "                                  learning_rate=parameterization[\"learning_rate\"],\n",
    "                                  max_delta_step=parameterization[\"max_delta_step\"],\n",
    "                                  max_depth=parameterization[\"max_depth\"],\n",
    "                                  min_child_weight=parameterization[\"min_child_weight\"],\n",
    "                                  n_estimators=parameterization[\"n_estimators\"],\n",
    "                                  reg_alpha=parameterization[\"reg_alpha\"],\n",
    "                                  reg_lambda=parameterization[\"reg_lambda\"],\n",
    "                                  scale_pos_weight=parameterization[\"scale_pos_weight\"],\n",
    "                                  subsample=parameterization[\"subsample\"]\n",
    "                                 )\n",
    "    \n",
    "    # learn the model\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # predict data\n",
    "    y_pred = xgb_model.predict(X_test)   \n",
    "    \n",
    "    # get accuracy of prediction\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print('accuracy:', accuracy)\n",
    "    return {'accuracy': (accuracy, 0.0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search space - set of input model parameters with allowed values\n",
    "\n",
    "xgb_search_space = SearchSpace(parameters=[\n",
    "    RangeParameter(\n",
    "        name='colsample_bylevel', parameter_type=ParameterType.FLOAT, lower=0.01, upper=1.0, log_scale=False\n",
    "    ),\n",
    "    RangeParameter(\n",
    "        name='colsample_bytree', parameter_type=ParameterType.FLOAT, lower=0.01, upper=1.0, log_scale=False\n",
    "    ),    \n",
    "    RangeParameter(\n",
    "        name='gamma', parameter_type=ParameterType.FLOAT, lower=1e-9, upper=0.5, log_scale=True\n",
    "    ),\n",
    "    RangeParameter(\n",
    "        name='learning_rate', parameter_type=ParameterType.FLOAT, lower=0.01, upper=1.0, log_scale=True\n",
    "    ),\n",
    "    RangeParameter(\n",
    "        name='max_delta_step', parameter_type=ParameterType.INT, lower=0, upper=20, log_scale=False\n",
    "    ),\n",
    "    RangeParameter(\n",
    "        name='max_depth', parameter_type=ParameterType.INT, lower=0, upper=50, log_scale=False\n",
    "    ),\n",
    "    RangeParameter(\n",
    "        name='min_child_weight', parameter_type=ParameterType.INT, lower=0, upper=10, log_scale=False\n",
    "    ),\n",
    "    RangeParameter(\n",
    "        name='n_estimators', parameter_type=ParameterType.INT, lower=50, upper=150, log_scale=False\n",
    "    ),\n",
    "    RangeParameter(\n",
    "        name='reg_alpha', parameter_type=ParameterType.FLOAT, lower=1e-9, upper=1.0, log_scale=True\n",
    "    ),\n",
    "    RangeParameter(\n",
    "        name='reg_lambda', parameter_type=ParameterType.FLOAT, lower=1e-9, upper=1000.0, log_scale=True\n",
    "    ),\n",
    "    RangeParameter(\n",
    "        name='scale_pos_weight', parameter_type=ParameterType.FLOAT, lower=1e-6, upper=500.0, log_scale=True\n",
    "    ),\n",
    "    RangeParameter(\n",
    "        name='subsample', parameter_type=ParameterType.FLOAT, lower=0.4, upper=0.6, log_scale=True\n",
    "    )    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleExperiment can be used here instead of Experiment because points tried in optimization\n",
    "# are computed synchrously via the evaluation function.\n",
    "\n",
    "exp = SimpleExperiment(\n",
    "    name='XGB optimalization',\n",
    "    search_space=xgb_search_space,\n",
    "    evaluation_function=xgb_evaluation_function,\n",
    "    objective_name='accuracy',\n",
    "    minimize=False # maximize accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Sobol initialization trials...\n",
      "Running GP+EI optimization trial 1/20...\n",
      "accuracy: 0.5060769230769231\n",
      "accuracy: 0.8226153846153846\n",
      "accuracy: 0.5060769230769231\n",
      "accuracy: 0.8805384615384615\n",
      "accuracy: 0.5060769230769231\n",
      "Running GP+EI optimization trial 2/20...\n",
      "accuracy: 0.8703846153846154\n",
      "Running GP+EI optimization trial 3/20...\n",
      "accuracy: 0.8431538461538461\n",
      "Running GP+EI optimization trial 4/20...\n",
      "accuracy: 0.8837692307692308\n",
      "Running GP+EI optimization trial 5/20...\n",
      "accuracy: 0.836076923076923\n",
      "Running GP+EI optimization trial 6/20...\n",
      "accuracy: 0.8755384615384615\n",
      "Running GP+EI optimization trial 7/20...\n",
      "accuracy: 0.8806923076923077\n",
      "Running GP+EI optimization trial 8/20...\n",
      "accuracy: 0.8367692307692308\n",
      "Running GP+EI optimization trial 9/20...\n",
      "accuracy: 0.8416923076923077\n",
      "Running GP+EI optimization trial 10/20...\n",
      "accuracy: 0.872\n",
      "Running GP+EI optimization trial 11/20...\n",
      "accuracy: 0.8756153846153846\n",
      "Running GP+EI optimization trial 12/20...\n",
      "accuracy: 0.886\n",
      "Running GP+EI optimization trial 13/20...\n",
      "accuracy: 0.8829230769230769\n",
      "Running GP+EI optimization trial 14/20...\n",
      "accuracy: 0.8842307692307693\n",
      "Running GP+EI optimization trial 15/20...\n",
      "accuracy: 0.8830769230769231\n",
      "Running GP+EI optimization trial 16/20...\n",
      "accuracy: 0.8889230769230769\n",
      "Running GP+EI optimization trial 17/20...\n",
      "accuracy: 0.874\n",
      "Running GP+EI optimization trial 18/20...\n",
      "accuracy: 0.8925384615384615\n",
      "Running GP+EI optimization trial 19/20...\n",
      "accuracy: 0.8923076923076924\n",
      "Running GP+EI optimization trial 20/20...\n",
      "accuracy: 0.8914615384615384\n"
     ]
    }
   ],
   "source": [
    "# We only instantiate the Sobol generator once, as the underlying model does not to be re-fit every \n",
    "# time new data is added to the experiment.\n",
    "\n",
    "sobol = modelbridge.get_sobol(search_space=exp.search_space)\n",
    "print(f\"Running Sobol initialization trials...\")\n",
    "\n",
    "for _ in range(5):\n",
    "    exp.new_trial(generator_run=sobol.gen(1))\n",
    "\n",
    "steps=20\n",
    "\n",
    "# GP=Gaussian Process, EI=Expected Improvement\n",
    "for i in range(steps):\n",
    "    print(f\"Running GP+EI optimization trial {i+1}/{steps}...\")\n",
    "    # Since we need to re-fit the underlying GP model, we reinstantiate the GP+EI model every\n",
    "    # time new data is added to the experiment.\n",
    "    gpei = modelbridge.get_GPEI(experiment=exp, data=exp.eval())\n",
    "    generator_run = gpei.gen(1)\n",
    "    # best_arm, _ = generator_run.best_arm_predictions\n",
    "    exp.new_trial(generator_run=generator_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_predictions: ({'colsample_bylevel': 0.5474285959685485, 'colsample_bytree': 0.45620725714512717, 'gamma': 2.3406682788971863e-07, 'learning_rate': 0.0865569197680857, 'max_delta_step': 15, 'max_depth': 6, 'min_child_weight': 6, 'n_estimators': 120, 'reg_alpha': 1.0, 'reg_lambda': 1.2224163336215744e-06, 'scale_pos_weight': 0.9618280388126623, 'subsample': 0.493838999516407}, ({'accuracy': 0.8925384283698093}, {'accuracy': {'accuracy': 1.4957462777355936e-09}}))\n"
     ]
    }
   ],
   "source": [
    "from ax.service.utils.best_point import get_best_from_model_predictions\n",
    "\n",
    "model_predictions = get_best_from_model_predictions(experiment=exp)\n",
    "print ('model_predictions:', model_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
